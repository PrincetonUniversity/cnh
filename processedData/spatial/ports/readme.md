# steps in getting port level attributes

1. **Find port lat/lons** (`make_all_ports.R`): this only returns port names and lat-lon. Returns `all_ports.csv`. Uses tickets to find `pcid`s of landed trips and `wc_fishing_communities.csv` which originally came from the Pacific Council's `kmz` files and `pcid.csv` which is the list of `pcid` from the PacFin website. Script
      + merges the names, lat/lon and `pcid` together
      + for ports that have a slash (i.e. ilwaco/chinook) use the lat/lon listed for each in port list and take average. There are only 3.
      + for those that do have PCID but don't have lat/lon, use `ggmap` to search google maps. To do this I remove all ports that have an "other" in their name (i.e. "other santa cruz and monterey county ports") because that's not specific enough to find lat/lon. Left with things like "bellingham bay" which is definitely a place.
      + add states for the missing ones to distinguish between newport, OR and newport, CA, for example.
2. **`make_port_polys.R`**: make polygon buffers of some radius to use to find landscape covariates. Takes function `radius` which is radius in meters of circle. Also overlaps this polygon with `WC` coastline to only look at water. Results are complicated around Puget Sound (will get some pacific ocean and some puget sound). But am leaving for consistency. Also saves `pcid`, the radius of the polygon, and area of on-sea in km2.
3. **Find covariates based on those ports** (all in `port_covariates/`)
      + **`n_processorIDs.R`**: find number of processors (`proceessorid`), uses the `all_ports.csv` and fish tickets. Calculates the annual average number (counts each year and takes average). Returns `n_processorIDs.csv` that has `pcid` and average number.
      + **`dist_big_city.R`**: find distance to major city. Is a function that can take the minimum population size as argument. Run it with 100,000 and 50,000. Saves `dist_big_city_pop[min_pop].csv`
      + **`substrate_types.R`**: uses `all_ports.csv` for lat/lon of ports. Then uses `extract()` with `buffer = 100 *1000` to find substrate within that radius. Too slow to run locally, runs on `della` in 1 hr 20 min. Result is `substrate_all.csv`  which has both the amount of cells of each substrate (so will be in m2) and the percent. For the percent, it's including 0 (no data) as a class. So the percents for the substrate types are lower. This is important because some ports (like tacoma) have no overlap with the raster (i.e. south Puget Sound is the "on the water" area, and layer isn't there). So need to decide how I want to measure percents. And whether the area that is covered by the layer is important (i.e. lower areas, less access to Pacific Ocean?). But even this is problematic because some of the farther north ports (i.e. anacortas) there's not a lot over overlap with habitat layer, but they're still on the water. Then `clean_up_substrate.R` does the following processing
          + calculate each ports percent (not including 0) and diversity of landscape types. flag ports which have less than 1e7 m2 of total measured habitat area, which suggests that there's not good coverage with habitat layer.
          + calculate also percent of rocks, since substrate 1 and 4 are both rocks (but 4 is inferred rocks). Also calculate Shannon Weiner diversity of habitat area.
          + generates final data which is `substrate_all_clean.csv`
      + **find distance to deep water**: `depth_dist.R`, loads `all_ports.csv`, reprojects to depth polygons from EFH, and uses `gDistance()` to find minimum cartesian distance in meters. Calculate for both lower and upper slope.
      + **calculate percent cover of MPAs for each polygon**
